{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a9d0463e",
      "metadata": {
        "id": "a9d0463e"
      },
      "source": [
        "# Neural Semantic Search Engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e35b77af",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e35b77af",
        "outputId": "aeda4960-4425-4c1a-e692-bfdc9a83ac44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.0)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.56.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.34.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.8.3)\n",
            "--- Running Semantic Search Project ---\n",
            "Loading Sentence-Transformer model: all-MiniLM-L6-v2...\n",
            "Generating embeddings for documents...\n",
            "Embeddings generated with shape: (5, 384)\n",
            "Creating a FAISS index with dimension 384...\n",
            "Index created with 5 vectors.\n",
            "\n",
            "Searching for query: 'What is artificial intelligence about?'\n",
            "\n",
            "--- Search Results ---\n",
            "Score: 0.7304\n",
            "Document: Machine learning is a subset of artificial intelligence.\n",
            "\n",
            "Score: 1.2591\n",
            "Document: Deep learning models are a type of neural network with many layers.\n",
            "\n",
            "Score: 1.7636\n",
            "Document: A dog is a man's best friend, known for its loyalty and companionship.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install sentence-transformers faiss-cpu numpy pandas\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import os\n",
        "\n",
        "\n",
        "def load_and_preprocess_data(file_path):\n",
        "    \"\"\"\n",
        "    Loads text data from a file and preprocesses it for semantic search.\n",
        "    This function can be extended to handle JSON, CSV, or other formats.\n",
        "    \"\"\"\n",
        "    documents = []\n",
        "    metadata = []\n",
        "\n",
        "\n",
        "    if file_path.endswith('.txt'):\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "\n",
        "            chunks = [p.strip() for p in content.split('\\n\\n') if p.strip()]\n",
        "            for i, chunk in enumerate(chunks):\n",
        "                documents.append(chunk)\n",
        "                metadata.append({\"source\": os.path.basename(file_path), \"chunk_id\": i})\n",
        "\n",
        "\n",
        "\n",
        "    if not documents:\n",
        "        raise ValueError(\"No documents found in the provided file path.\")\n",
        "\n",
        "    return documents, metadata\n",
        "\n",
        "\n",
        "def generate_embeddings(documents, model_name='all-MiniLM-L6-v2'):\n",
        "    \"\"\"\n",
        "    Uses a pre-trained Sentence-Transformer model to generate embeddings.\n",
        "    'all-MiniLM-L6-v2' is a fast and effective model for many use cases.\n",
        "    For more complex tasks, you might use 'multi-qa-mpnet-base-dot-v1'\n",
        "    or a larger model.\n",
        "    \"\"\"\n",
        "    print(f\"Loading Sentence-Transformer model: {model_name}...\")\n",
        "    model = SentenceTransformer(model_name)\n",
        "    print(\"Generating embeddings for documents...\")\n",
        "    corpus_embeddings = model.encode(documents, convert_to_numpy=True)\n",
        "    print(f\"Embeddings generated with shape: {corpus_embeddings.shape}\")\n",
        "    return corpus_embeddings, model\n",
        "\n",
        "\n",
        "def create_faiss_index(embeddings):\n",
        "    \"\"\"\n",
        "    Creates a FAISS index for efficient similarity search.\n",
        "    We'll use IndexFlatL2 for a simple, brute-force search.\n",
        "    For very large datasets (millions of vectors), more advanced\n",
        "    indexes like IndexIVFFlat or IndexHNSWFlat should be used.\n",
        "    \"\"\"\n",
        "    d = embeddings.shape[1]\n",
        "    print(f\"Creating a FAISS index with dimension {d}...\")\n",
        "    index = faiss.IndexFlatL2(d)\n",
        "    index.add(embeddings)\n",
        "    print(f\"Index created with {index.ntotal} vectors.\")\n",
        "    return index\n",
        "\n",
        "\n",
        "def semantic_search(query, model, index, documents, k=5):\n",
        "    \"\"\"\n",
        "    Performs a semantic search on the FAISS index.\n",
        "    \"\"\"\n",
        "    print(f\"\\nSearching for query: '{query}'\")\n",
        "    query_embedding = model.encode([query])\n",
        "    D, I = index.search(query_embedding, k)\n",
        "\n",
        "    results = []\n",
        "    for i in range(k):\n",
        "        doc_idx = I[0][i]\n",
        "        score = D[0][i]\n",
        "        results.append({\n",
        "            \"score\": score,\n",
        "            \"document\": documents[doc_idx]\n",
        "        })\n",
        "    return results\n",
        "\n",
        "\n",
        "def run_semantic_search_project():\n",
        "    \"\"\"\n",
        "    Combines all steps into a single, executable pipeline.\n",
        "    \"\"\"\n",
        "    print(\"--- Running Semantic Search Project ---\")\n",
        "\n",
        "\n",
        "    dummy_file_path = \"sample_corpus.txt\"\n",
        "    if not os.path.exists(dummy_file_path):\n",
        "        with open(dummy_file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(\"The quick brown fox jumps over the lazy dog.\\n\\n\")\n",
        "            f.write(\"A dog is a man's best friend, known for its loyalty and companionship.\\n\\n\")\n",
        "            f.write(\"Machine learning is a subset of artificial intelligence.\\n\\n\")\n",
        "            f.write(\"Deep learning models are a type of neural network with many layers.\\n\\n\")\n",
        "            f.write(\"A fox is a clever and cunning animal, often found in forests.\\n\\n\")\n",
        "\n",
        "\n",
        "    documents, metadata = load_and_preprocess_data(dummy_file_path)\n",
        "\n",
        "\n",
        "    corpus_embeddings, model = generate_embeddings(documents)\n",
        "\n",
        "\n",
        "    faiss_index = create_faiss_index(corpus_embeddings)\n",
        "\n",
        "\n",
        "    user_query = \"What is artificial intelligence about?\"\n",
        "    results = semantic_search(user_query, model, faiss_index, documents, k=3)\n",
        "\n",
        "    print(\"\\n--- Search Results ---\")\n",
        "    for result in results:\n",
        "        print(f\"Score: {result['score']:.4f}\")\n",
        "        print(f\"Document: {result['document']}\\n\")\n",
        "\n",
        "run_semantic_search_project()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}